import streamlit as st
from langchain_core.messages import HumanMessage
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os
from PIL import Image
import pytesseract
from fugashi import Tagger

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# ÄÆ°á»ng dáº«n Tesseract (tÃ¹y mÃ¡y báº¡n)
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# Init MeCab
tagger = Tagger()

# Init Groq LLM
model = ChatGroq(
    groq_api_key=os.getenv("GROQ_API_KEY"),
    model="meta-llama/llama-4-scout-17b-16e-instruct",
    temperature=0
)

# Streamlit UI
st.set_page_config(page_title="AI há»c tiáº¿ng Nháº­t", page_icon="ğŸ‡¯ğŸ‡µ", layout="centered")
st.title("ğŸ‡¯ğŸ‡µ AI OCR + Dá»‹ch + ChÃº thÃ­ch há»c tiáº¿ng Nháº­t")

uploaded_file = st.file_uploader("Táº£i lÃªn áº£nh tiáº¿ng Nháº­t (png, jpg, jpeg)", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    # OCR vá»›i Tesseract
    image = Image.open(uploaded_file)
    japanese_text = pytesseract.image_to_string(image, lang="jpn", config="--psm 6")

    st.subheader("ğŸ“„ VÄƒn báº£n OCR Ä‘Æ°á»£c")
    st.text_area("Ná»™i dung", japanese_text, height=200)

    # NÃºt dá»‹ch + phÃ¢n tÃ­ch
    if st.button("â¡ï¸ Dá»‹ch & ChÃº thÃ­ch (gá»™p chung)"):
        with st.spinner("â³ Äang xá»­ lÃ½..."):
            # PhÃ¢n tÃ­ch MeCab trÆ°á»›c
            words = []
            for word in tagger(japanese_text):
                surface = word.surface
                reading = word.feature.kana if hasattr(word.feature, "kana") else ""
                pos = word.feature.pos if hasattr(word.feature, "pos") else ""
                words.append(f"{surface} ({reading}) - {pos}")

            mecab_analysis = "\n".join(words)

            # Gá»­i sang LLM: cáº£ Ä‘oáº¡n + phÃ¢n tÃ­ch
            response = model.invoke([
                HumanMessage(
                    content=f"""
HÃ£y dá»‹ch Ä‘oáº¡n vÄƒn tiáº¿ng Nháº­t nÃ y sang tiáº¿ng Viá»‡t vÃ  Ä‘á»“ng thá»i lÃ m báº£ng chÃº thÃ­ch há»c tiáº¿ng Nháº­t (Kanji/Hiragana/Ã nghÄ©a).
VÄƒn báº£n OCR:
{japanese_text}

PhÃ¢n tÃ­ch tá»« vá»±ng (MeCab):
{mecab_analysis}
""",
                    additional_kwargs={"job_role": "Japanese language teacher"}
                )
            ])

        st.subheader("ğŸ“– Báº£n dá»‹ch + ChÃº thÃ­ch (gá»™p)")
        st.write(response.content)
        st.success("âœ… HoÃ n táº¥t!")


def clean_mecab(japanese_text):
    words = []
    buffer = ""

    for word in tagger(japanese_text):
        surface = word.surface
        reading = word.feature.kana if hasattr(word.feature, "kana") else ""
        pos = word.feature.pos if hasattr(word.feature, "pos") else ""

        # Loáº¡i bá» kÃ½ tá»± vÃ´ nghÄ©a nhÆ° dáº¥u cháº¥m cÃ¢u, kÃ½ tá»± rá»i láº»
        if pos in ["è¨˜å·", "ç©ºç™½"]:
            if buffer:
                words.append(buffer)
                buffer = ""
            continue

        # Gom katakana liÃªn tiáº¿p
        if surface.iskatakana():
            buffer += surface
        else:
            if buffer:
                words.append(buffer)
                buffer = ""
            words.append(surface)

    if buffer:
        words.append(buffer)

    return words